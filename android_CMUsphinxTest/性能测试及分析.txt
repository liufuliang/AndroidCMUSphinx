2014-3-10
	1.用开源的CMU SPHINX代码生成的DEMO，简单看了下JAVA部分，具体流程
		1.>主界面PocketSphinxDemo oncreate ，注册RecognizerTask Runnable 到 Thread，启动thread.start(Runnable.run()...),注册按钮按下开始录音，随后开始解码--改变Runnable的状态
		2.>解码分三部分，首先解码前 Decoder.startUtt    然后有数据的时候 Decoder.processRaw   并且获得部分结果 Decoder.getHyp   最后 Decoder.endUtt  并且获得最终结果 Decoder.getHyp
		3.>Decoder.processRaw 调用 JNI 层 Java_edu_cmu_pocketsphinx_pocketsphinxJNI_Decoder_1processRaw_1_1SWIG_11 经测试，其主要消耗时间为 
			Decoder_processRaw__SWIG_1 ->  ps_process_raw  又分为  提取数据特征 ps_process_raw->acmod_process_raw  和 数据查找  ps_process_raw->ps_search_forward
			
	2.数据源提取数据特征（音素），其与 8K采样率还是 16K 采样率 或者其他采样率无关（有待验证，可能会影响提取数据特征的速度），最好采用16K的采样率
	
2014-3-11  针对相同的语音数据
	1.将目前的录音测试，改成添加测试数据进行测试，以保证每次测试的数据都是一样的，方便判断测试效果。 测试语音--你好百度浏览器
	2.经测试发现  processRaw单次处理的数据如果长一些的话，会节省一些时间（测试由 1024  加大到 4096 ，总的时间由 244ms 减少到 238 ms）
	3.acmod_process_raw--主要消耗时间的函数 fe_process_frames （70% -- 90%）  acmod_process_mfcbuf（10%--20%）
	
2014-3-12
	1.fe_process_frames--主要消耗时间的函数 fe_process_frames 002  fe_process_frames 003
		fe_read_frame fe_write_frame fe_shift_frame
	2.查阅代码发现 提取特征值的时候 默认使用的是定点运算，在ANDROID平台将浮点数转换成定点来使用。
		尝试将默认定点数改成 浮点数，发现中文大数据识别率明显提升，但是速度变慢。	（将 sphinx_config.h 和 config.h 中的 整数开关FIXED16 和 定点数开关都关闭，即代表打开了浮点开关）
		尝试将默认定点数改成 整数，发现即使使用小数据库，依然无法正常识别数据。		FIXED16
		稳妥起见，依然使用默认的定点开关。 FIXED_POINT
	3.添加汇编提调试手段  gnu_asm_debug.h
	
2014-3-13
	1.fe_read_frame 函数宏 SWAP_INT16 汇编化  效果不明显
		测试汇编化之前，整儿解码过程耗时 300 -- 350 ms
		测试汇编化之后，整儿解码过程耗时 290 -- 350 ms
	2.fe_read_frame 函数宏 SWAP_INT32 汇编化  效果不明显
	3.fe_pre_emphasis 函数汇编化 
	
2014-3-14
	1.fe_hamming_window_arm_code 函数 汇编化  
		运行异常，WINDOW变量地址被破坏
		
2014-3-17
	1.fe_hamming_window_arm_code 函数 汇编化 
		@ 不知道问什么，调用C程序破坏了R2的值，所以需要记录，然后回复
		从新修改成C调用汇编，而不用汇编调用C代码，简单测试大概提高效率，20MS--30MS之间
		
2014-3-18
	1.将 结构体 fe_t 映射到 汇编程序中，以便提高访问速度
	2.fe_fft_real 汇编化
	
2014-3-19
	1.fe_fft_real 汇编化  增加汇编全局变量，以便减少寻址
	
2014-3-20
	1.fe_fft_real 汇编化  将怎加的全局变量修改到数据区，否则访问异常，只能读，不能写
	
2014-3-22
	1.fe_fft_real 汇编化代码，调试
	
	
2014-3-24
	1.fe_fft_real 汇编化代码，调试  增加调试方法，存储调试信息到文件
	2.调试过程中使用向文件打LOG的方式，然后对比结果(使用C和汇编的运行结果)
	
2014-3-25
	1.fe_fft_real 汇编化完成，270ms ---290ms  相比之前大概提高了 10MS(280MS -- 300 MS)  
	以上测试都是用的自己的联想手机 1.3G CPU 1G RAM
	2. fe_mel_spec 汇编化代码
	
2014-3-26
	1. fe_mel_spec 汇编化代码 测试OK.效率提升到 258ms -- 280ms
	2. fe_dct2 汇编化代码
	
2014-3-27
	1. fe_dct2 汇编化代码 260MS左右
	2. 经测试，发现 fe_lifter 因为没有设置，所以可以先不做有优化
	
2014-3-31
	1.测试 ps_search_forward 函数发现 主要消耗在运行 ps_search_step 函数， ps_search_step 为函数指针，
		经测试 ngram_funcs->ngram_search_step 356次
			phone_loop_search_funcs->phone_loop_search_step 356次
			
2014-4-1
	1. 模型与匹配
	2.	phone_loop_search_step -> acmod_activate_hmm 
		acmod_score 函数会有读写文件的操作，考虑改成读写内存的方式--经测试并未读写文件，但是时间消耗确实比较大。。。。。。		
	
2014-4-2
	1. 添加 结构体 hmm_context_s hmm_s acmod_s
	2. acmod_activate_hmm 汇编化完成
	
2014-4-3
	1. SWIGEXPORT jlong JNICALL Java_edu_cmu_pocketsphinx_pocketsphinxJNI_Decoder_1getHyp(JNIEnv *jenv, jclass jcls, jlong jarg1, jobject jarg1_) {
		SWIGINTERN Hypothesis *Decoder_getHyp(Decoder *self)
		
	2.acmod_score 函数在前四五次会及其消耗时间-----注意查找原因
		acmod_score -> ps_mgau_frame_eval 占比最大整个函数的90%以上   ps_mgau_frame_eval--> s2_semi_mgau_frame_eval
		acmod_score -> acmod_flags2list 占比很少，其他部分可以忽略 --简单做了C代码级优化
	
2014-4-8
	1.phone_loop_search_step->evaluate_hmms -> hmm_vit_eval -> hmm_vit_eval_anytopo  继续深入
	
2014-4-9
	1.	phone_loop_search_step->evaluate_hmms
		phone_loop_search_step->prune_hmms
		phone_loop_search_step->phone_transition
		三个方法时间占比较小，简单做了C代码级优化
	2. 经过测试@ SP堆栈指针 逆向增长 所以可以在站上开辟空间操作
	
vcyber_print_sp_address:
	stmfd 	sp!,{r0-r1,lr}
	debug_output_sp_addr	sp
	mov		r0,#0xaa
	str		r0,[sp,#-0x10]
	str		r0,[sp,#-0x14]
	str		r0,[sp,#-0x18]
	str		r0,[sp,#-0x1c]
	ldmfd 	sp!,{r0-r1,pc}
	
2014-4-10
	1.	s2_semi_mgau_frame_eval->get_scores_4b_feat->get_scores_4b_feat_4 函数汇编化
		利用堆栈获得第五个参数，在利用压栈和出栈保存临时变量
		
2014-4-11
	1.	get_scores_4b_feat_4 汇编化完成
	
2014-4-14
	1.	目前需要的时间进过测试为220MS--30MS，原始版本大概是290ms--330ms
	
2014-4-15
	1. 	Java_edu_cmu_pocketsphinx_pocketsphinxJNI_new_1Decoder_1_1SWIG_11	->
		new_Decoder__SWIG_1 ->
		ps_init -> 		可以考虑直接调用这一层函数解析声音数据 PocketSphinx
		ps_reinit ->   
		acmod_init ->初始化流程 
		
		
		ps_search_init 
		
		
2014-4-16
	1.	acmod_init - cmd_ln_parse_file_r -featparams 解析FEAT参数文件 成相关参数 acmod->config
	2. 	acmod_init - fe_init_auto_r 初始化一个新的前段 FE
	3.	acmod_init - acmod_init_feat 初始化一个FCB	
	4.	acmod_init - acmod_init_am - 
			加载声学模型文件	-mdef	mdef 到 acmod->mdef
			加载声学模型文件	-tmat	tmat 到 acmod->tmat
			加载声学模型文件	-mean	mean 到 acmod->mgau
			加载声学模型文件	-var	var 到 acmod->mgau
			加载声学模型文件	-senmgau	senmgau 到 acmod->mgau
			 
	
2014-4-17,18
	1.	ps_reinit - dict_init 字典数据文件加载  都加载到HASH表中，哈希表的大小
		ps->acmod 	声学模型
		ps->dict 	词典文件
		ps->search 	语言模型
	2.  decoder -> 
		startUtt ->
		Java_edu_cmu_pocketsphinx_pocketsphinxJNI_Decoder_1startUtt_1_1SWIG_11 ->
		Decoder_startUtt__SWIG_1 ->
		ps_start_utt -> 可以考虑直接调用这一层函数解析声音数据 PocketSphinx
				acmod_start_utt 
				ps_search_start -> ngram_search_start -> ngram_model_flush
														ngram_fwdtree_start
														ngram_fwdflat_start
	3. 	ngram_model_init() -> ngram_model_set_funcs->flush -> ngram_model_set_flush
				
2014-4-21
	1.ps_end_utt -> ps_search_finish == ngram_search_finish
	ngram_search_finish ->
	ngram_fwdflat_search ->	循环执行356次
	acmod_score ->
	ps_mgau_frame_eval ->s2_semi_mgau_frame_eval
	s2_semi_mgau_frame_eval	循环3次执行一下函数
			mgau_dist
				eval_topn
				eval_cb
			mgau_norm
			
2014-4-22
	mgau_norm 汇编化
	
2014-4-28
2014-4-29

2014-4-30
	eval_topn 汇编化
	
2014-5-4
2014-5-5
	eval_topn 汇编化 调试OK,性能提升比较明显
	用华为G510,双核1.2G，512M内存，进行测试，并且测试了之前汇编化代码的效率。
2014-5-10
	eval_cb,汇编化，调试OK,性能提升比较明显
	
	
	acmod_score -> calc_feat_idx
		/* Calculate position of requested frame in circular buffer. */
		calc_feat_idx
	
	ps_mgau_frame_eval->s2_semi_mgau_frame_eval  注意这个参数 acmod->feat_buf[feat_idx]
	
	
	
	
	
	
	
	
	0.acmod_process_raw -- 处理声音数据，提取特征值，
		fe->mel_fb->num_filters  每帧的特征值个数，      fe_mel_spec fe->mel_fb->num_filters == 20
		acmod->mfc_buf 特征值存储缓冲区
		
	1.ps_search_forward--主要消耗时间的函数 大概是 acmod_process_raw 时间的 2.5 倍 到 3 倍
	
	2.优先处理对模型文件的操作，理论上最影响效率的是IO操作
	
	
	
2015-1-12
	多识别任务，内存持续增长
	
	
2015嵌入式识别引擎的工作计划 2015-1-1
	按优先级排列：
	1.解决手机端测试数据增大，导致的引擎崩溃的问题。
	2.解决引擎内存增长的问题
	3.测试各种参数对引擎性能的影响
	4.测试引擎在各种性能移动端的性能表现
	5.模型的浮点转定点
	6.引擎词典的优化，根据语法文件自动分词之后，生成新的词典
	7.引擎速度的持续优化
	
	
2015-1-26
	测试各种参数对引擎性能的影响
	
	/** Minimal set of command-line options for PocketSphinx. */
#define POCKETSPHINX_OPTIONS \
    waveform_to_cepstral_command_line_macro(), \	//波形到倒谱 命令行宏
    cepstral_to_feature_command_line_macro(), \		//倒谱到特征 命令行宏
    POCKETSPHINX_ACMOD_OPTIONS, \					//声学模型 声学建模的命令行选项
        POCKETSPHINX_BEAM_OPTIONS,   \				//定义调整搜索的波束宽度的参数选择
        POCKETSPHINX_SEARCH_OPTIONS, \				//定义调整搜索的其他参数的参数选择
        POCKETSPHINX_DICT_OPTIONS, \				//词典
        POCKETSPHINX_NGRAM_OPTIONS, \				//统计语言模型
        POCKETSPHINX_FSG_OPTIONS, \					//有限状态语法的命令行选项。
        POCKETSPHINX_DEBUG_OPTIONS,	\				//调试,日志记录选项。
        CYVOICE_E_OPTIONS
        
   
   
#define S3_MAX_FRAMES		15000    /* RAH, I believe this is still too large, but better than before */   

15000 * 0.025 =   375 s = 375 / 60 = 6.2 minute
测试数据最大为六分钟多，



2015-3-10
	ps_process_raw	//处理数据
	{
		while(n_samples)
		{
			acmod_process_raw	//转换部分声音数据为声音特征（梅尔频谱倒数） Process some data into features.
			{
				fe_process_frames	//分帧
				{
					//检查声音特征存储缓冲区
					
					//检查数据长度，要大于一定帧数 即（8000 | 16000）* 0.025 = 一帧数据长度
					//如果当前数据与溢出缓冲区的数据和不足一帧，则将数据写入溢出缓冲区，以备下次来新数据继续使用
					
					//0
					fe_read_frame	//读第一帧数据 从数据下标 0 开始，并且是整帧，即一帧数据长度  8000频率时数据为200个
					{
						//Swap and dither if necessary 
						//根据需要对数据进行大小端变换，即交换，若需要，对数据进行随机抖动，避免全零的静音情况
						
						//分帧
						//研究发现，在10～30ms的范围内，语音频谱特征和一些物理特征参数基本保持不变。
						//因此可以将平稳过程的处理方法和理论引入到语音信号的短时处理中，将语音信号划分为很多短时的语音段，每个短时的语音段称为一个分析帧。
						//如果帧和帧之间都是连续的，那会不会出现帧和帧之间的信息可能会得不到处理的机会，为了避免这种情况，
						//在分帧时需要确定一个帧移，也就是帧是叠加的，使帧和帧之间能够平滑的过渡，帧移一般都是取0~1/2之间的某一个值。我一般取2/5
						fe_spch_to_frame //对数据进行分帧,加窗处理-----------------------
						{
							fe_pre_emphasis //数据预加重，并且将数据从声音缓冲区（p_spch）拷贝到 帧缓冲区（p_frame）
							{
								for (i = 1; i < len; i++)
	        					out[i] = (frame_t) in[i] - (frame_t) in[i-1] * factor;
							}
							
							
							/* Zero pad up to FFT size. */ 为后续傅里叶变换做准备
						    memset(p_frame + len, 0,(fe->fft_size - len) * sizeof(*p_frame));
							
							
							//加窗
							//我们的帧在起始和结束肯定是会出现不连续情况的，那样这个信号在分帧之后，就会越来越背离原始信号，
							//此时我们需要对信号进行加窗处理，目的很明显了，就是为了减少帧起始和结束的地方信号的不连续性问题
							//加窗之后是为了进行傅里叶展开，所以加窗的目的大致如下：
							//	使全局更加连续，避免出现吉布斯效应。
							//	加窗之后，原本没有周期性的语音信号呈现出周期函数的部分特征。
							//hamming窗 公式 = 0.54 - 0.46 * cos（2 * π * i / (n - 1)）
							fe_hamming_window
						}
						
					    
					}
					
					//生成梅尔频谱特征（MFCC）
					fe_write_frame
					{
						//计算特征大小
						fe_spec_magnitude
						{
							//
							fe_fft_real
							
							//特征 fe->spec
							spec[j] = fft[j] * fft[j] + fft[fftsize - j] * fft[fftsize - j];
						}
						
						//计算梅尔滤波器特征值 
						fe_mel_spec
						{
							//梅尔滤波器个数 20
							num_filters = 20
							
							//当前滤波器 whichfilt 的宽度
							fe->mel_fb->filt_width[0] = 8
							fe->mel_fb->filt_width[1] = 10
							fe->mel_fb->filt_width[2] = 11
							fe->mel_fb->filt_width[3] = 11
							fe->mel_fb->filt_width[4] = 13
							fe->mel_fb->filt_width[5] = 14
							fe->mel_fb->filt_width[6] = 15
							fe->mel_fb->filt_width[7] = 17
							fe->mel_fb->filt_width[8] = 19
							fe->mel_fb->filt_width[9] = 20
							
							fe->mel_fb->filt_width[10] = 22
							fe->mel_fb->filt_width[11] = 24
							fe->mel_fb->filt_width[12] = 26
							fe->mel_fb->filt_width[13] = 29
							fe->mel_fb->filt_width[14] = 32
							fe->mel_fb->filt_width[15] = 35
							fe->mel_fb->filt_width[16] = 38
							fe->mel_fb->filt_width[17] = 42
							fe->mel_fb->filt_width[18] = 45
							fe->mel_fb->filt_width[19] = 49
							
							//梅尔滤波器参数特征
							mfspec
						}
						
						//梅尔频谱倒数，倒谱个数 num_filters = 13
						fe_mel_cep
						{
							//根据梅尔滤波器特征生成 长度 13 的频谱倒数
							mfspec --》mfcep
						}
						
						
						fe_lifter
						{
							//没有实际执行
						}
					}
					
					//处理非第一帧 非最后一帧，中间帧需要做数据位移，即第一帧使用数据从0开始，帧长200，
					//中间帧的实际帧长也是200，但是起始位置是从上一帧的80开始，也就是说有120的数据是和前一帧重叠的
					//1.2.3....
					fe_shift_frame
					{
						//处理数据 
						memmove(p_spch, p_spch + frame_shift,    offset * sizeof(*p_spch));
    					memcpy(p_spch + offset, in, len * sizeof(*p_spch));
    					
    					//对数据进行分帧,加窗处理
    					fe_spch_to_frame(fe, offset + len);
    					 
					}
					
					//生成梅尔频谱特征（MFCC）----梅尔频谱倒数
					fe_write_frame
					{
					
					}
					
					
					
					//end,处理最后的数据
				}
				
				

				//这部分不太清晰，如果可能在分析一遍
				acmod_process_mfcbuf	//生成MFCC
				{
					//处理倒谱
					acmod_process_cep
					{
						//找到特征BUFF的起始位置
						
						//如果上一次的还有数据，加上这次的大于能处理的数据量，则先处理一部分
						if (inptr + nfeat > acmod->n_feat_alloc)
							feat_s2mfc2feat_live
						
						//正常处理
						feat_s2mfc2feat_live
						{
							//倒谱均值归一化  Various forms of cepstral mean normalization
							feat_cmn
							{
								cmn_prior
								{
									
								}
								
								//打印倒谱特征  
								cep_dump_dbg
							}
							
							//自动增益控制  Various forms of automatic gain control 
							feat_agc
							{
								
							}
							
							//计算特征  将之前的频谱倒数 （13维向量） 计算出后续两个13维的向量
							fcb->compute_feat  -》feat_1s_c_d_dd_cep2feat
							{
								//0--12
								memcpy(feat[0], mfc[0], feat_cepsize(fcb) * sizeof(mfcc_t));
								
								//13 - 25
								for (i = 0; i < feat_cepsize(fcb); i++)
									feat[13 + i] = mfc[w] - mfc[-w], where w = FEAT_DCEP_WIN;
									
								//26 - 38
								for (i = 0; i < feat_cepsize(fcb); i++)
									feat[26 + i] = (mfc[w+1] - mfc[-w+1]) - (mfc[w-1] - mfc[-w-1]), where w = FEAT_DCEP_WIN;
							}
							
						}
					}
				}
			}
			
			
			
			ps_search_forward	//对特征值评分，并搜索结果  Score and search as much data as possible
			{
				while (acmod->n_feat_frame > 0) 
				{
					//满足条件，则评分，搜索
					if (acmod->output_frame >= pl_window)
					{
						//根据使用语法不同，通PS结构体的函数指针，例如fsg语法 -> fsg_search_step  NGRAM->ngram_search_step
						//以下根据 fsg_search_step 为例
						ps_search_step-> fsg_search_step
						{
							//Activate our HMMs for the current frame if need be
							//为当前帧激活隐马模型
							fsg_search_sen_active
							{
								//清除激活的音素向量表
								acmod_clear_active
								
								//激活节点
								for (gn = fsgs->pnode_active; gn; gn = gnode_next(gn))
									acmod_activate_hmm
							}
							
							//为当前帧计算混合高斯评分
							senscr = acmod_score
							{
								//计算绝对帧号
								calc_frame_idx(acmod, inout_frame_idx);
								
								//计算请求帧在特征帧的循环缓冲区中的位置
								calc_feat_idx(acmod, frame_idx)
								
								//如果有语音音素文件，则加载
								if (acmod->insenfh
								{
								}
								
								//创建激活的音素列表
								acmod_flags2list(acmod);
								{
									//总得音素数 5210
									total_dists = bin_mdef_n_sen(acmod->mdef);
									
									//块数 一块32
									total_words = total_dists >> 5;
									//不足一块 
									extra_bits = total_dists & 0x1f;
									
									//
									acmod->senone_active	//激活的音素缓冲区
									acmod->n_senone_active	//激活的音素个数
									
									//当前帧 acmod->output_frame 的激活音素数 acmod->n_senone_active
									E_DEBUG(1, ("acmod_flags2list: %d active in frame %d\n",
                							acmod->n_senone_active, acmod->output_frame));
								}
								
								
								//Generate scores for the next available frame
								//生成下一个可能帧的得分
								ps_mgau_frame_eval -> s2_semi_mgau_frame_eval
								{
									//首先清除所有音素的得分
									memset(senone_scores, 0, s->n_sen * sizeof(*senone_scores));
									
									//计算帧偏移以及特征偏移
									topn_idx = frame % s->n_topn_hist;
    								s->f = s->topn_hist[topn_idx];
    								
									//s->n_feat -- 三层特征
									for (i = 0; i < s->n_feat; ++i)
									{
										memcpy(s->f[i], lastf[i], sizeof(vqFeature_t) * s->max_topn);
										
										//高斯距离
										mgau_dist(s, frame, i, featbuf[i]);
										{
											//计算一帧声音特征（0--12,13--25,26--38）（第 feat（0--2） 分量）的高斯得分，并且提取其中TOP--N个结果
											//N==4  将前四个特征高斯得分结果保存到 vqFeature_t *topn; = s->f[feat];，并对前四个高斯得分排序  
											//struct vqFeature_s {
											//    int32 score; 			// [r0,#0x00]		/* score or distance */
											//    int32 codeword; 		// [r0,#0x04]		/* codeword (vector index) */
											//};
											eval_topn(s, feat, z);
											
											
											/* If this frame is skipped, do nothing else. */
											// "Frame GMM computation downsampling ratio"
											// s->ds_ratio 默认值 = 1，即每隔一帧跳过一次
										    if (frame % s->ds_ratio)
										        return;
										        
										     /* Evaluate the rest of the codebook (or subset thereof). */
										     //评估其他的码本，对密度（n_density == 256）范围内的高斯得分取最高的TOPN-N（4）个，并替换TOP-N的得分和 codeWord
										     eval_cb(s, feat, z);
										     {
										     	det = s->dets[feat];
    											detE = det + s->n_density;
    											
										     	//对密度（n_density == 256）范围内的高斯得分
										     	for (detP = det; detP < detE; ++detP)
										     	{
										     		d = *detP;
										     		
										     		//查找比最差得分好的结果
										     		for (j = 0; (j < ceplen) && (d >= worst_score); ++j)
										     		{
										     		}
										     		
										     		//得分已经比最差得分还小，所以提前退出上边的循环，导致 j < ceplen
										     		if (j < ceplen) {
											            /* terminated early, so not in topn */
											            mean += (ceplen - j);
											            var += (ceplen - j);
											            continue;
											        }
											        //异常情况，提前退出
											        if ((int32)d < worst_score)
            											continue;
            											
            										cw = detP - det;
            										//在TOP--N个 vqFeature_t 中插入上边提取出来满足条件的 CW(codeword) 和 d(score)
            										
            										
            										//存储位置
            										topn = s->f[feat];
										     	}
										     }
										
											//计算特征向量的高斯距离
											eval_topn
											{
												//通过已经计算出来的声音特征向量，计算高斯分布
												//并且挑选出评分最高的 topN 个
												//使用高斯得分上最大数量。topN     Maximum number of top Gaussians to use in scoring.
												for (j = 0; j < ceplen; j++) 	//ceplen = 13, 3 *13维的特征向量
												{
													diff = *obs++ - *mean++;
													
													sqdiff = diff * diff;
													
													compl = sqdiff *(*var);
													
													d = GMMSUB(d, compl);
												}
												
												topn[i].score
												
											}
											
											//评估码本（codebook）是否在高斯分布范围内
											eval_cb(s, feat, z);
											{
												//因为已经对可能的得分做了从高到低的排序
												//所以
												worst = topn + (max_topn - 1);
												
												//根据高斯分布的密度（256），评估当前帧音频可能的四种得分，其对应的码本的分布情况
												//detP依据高斯分布的得分
												for (detP = det; detP < detE; ++detP)
												{
													//计算
													//首先取出当前帧音频的最坏得分
													int32 worst_score = worst->score;
													
													
													//基准得分
													d = *detP;
													
													//依据基准得分的特征得分，当 满足 d < worst_score,退出循环
													for (j = 0; (j < ceplen) && (d >= worst_score); ++j) 
													{
														//
													}
													
													 /* terminated early, so not in topn */
													 //当出现 d < worst_score,结束当前循环
													if (j < ceplen) 
													{
														//处理均值和方差的偏移
														mean += (ceplen - j);
            											var += (ceplen - j);
            											continue;
													}
													
													//如果得分小于 最坏的得分，则也被排除掉
													if ((int32)d < worst_score)
            											continue;
            											
            										
            										//转换码本，查找当前码本值是否在 TOPN 的存储区内，如果存在TOPN内，不用再更新，插入
            										cw = detP - det;
            										for (i = 0; i < max_topn; i++) {
            										/* already there, so don't need to insert */
											            if (topn[i].codeword == cw)
											                break;
											        }
											        
											        if (i < max_topn)
            											continue;       /* already there.  Don't insert */
            
            										//满足条件，吧当前的码本以及得分赋给
            										for (cur = worst; cur >= (topn + 1) && (int32)d >= (cur-1)->score; --cur)
											            memcpy(cur, cur-1, sizeof(vqFeature_t));
											        //++cur;
													
											        cur->codeword = cw;
											        cur->score = (int32)d;
												}
												
											}
										}
										//高斯 归一化 
										s->topn_hist_n[topn_idx][i] = mgau_norm(s, i);
										{
											//量化的标准化常数计算
											//Compute quantized normalizing constant.
											norm = p_f[0].score >> SENSCR_SHIFT;
											
											/* Normalize the scores, negate them, and clamp their dynamic range. */
											for (j = 0; j < s->max_topn; ++j)
											{
												p_f[j].score = -((p_f[j].score >> SENSCR_SHIFT) - norm);
											}
											//计算量化的归一化常数 Compute quantized normalizing constant.
											norm = p_f[0].score >> SENSCR_SHIFT;
											
											// Normalize the scores, negate them, and clamp their dynamic range.
											// 正常化的分数，否定他们，他们的动态范围和夹具。
											
										}
										
										//获得当前帧的特征数据的高斯得分
										get_scores_4b_feat
										{
											//topn == 4
											get_scores_4b_feat_4
											{
												//Precompute scaled densities.
												//预计算规模的密度。
												//预计算密度分布
												for (j = 0; j < 16; ++j) 
												{	uint8 temp = mixw_cb[j];
											        w_den[j] = temp + f0_score;
											        w_den[16+j] = temp + f1_score;
											        w_den[32+j] = temp + f2_score;
											        w_den[48+j] = temp + f3_score;
											    }
											    
											    //获得权重，有限个数，根据 TOPN 决定
											    pid_cw0
											    pid_cw1
											    pid_cw2
											    pid_cw3
											    
											    //计算所有激活的音素的得分
											    for (l = j = 0; j < n_senone_active; j++) {
											    	int n = senone_active[j] + l;	//索引值都是基于前一个的偏移
											    	
											    	if (n & 1) {
											            cw = pid_cw0[n/2] >> 4;
											            tmp = w_den[0][cw];
											            cw = pid_cw1[n/2] >> 4;
											            tmp = fast_logmath_add(s->lmath_8b, tmp, w_den[1][cw]);
											            cw = pid_cw2[n/2] >> 4;
											            tmp = fast_logmath_add(s->lmath_8b, tmp, w_den[2][cw]);
											            cw = pid_cw3[n/2] >> 4;
											            tmp = fast_logmath_add(s->lmath_8b, tmp, w_den[3][cw]);
											        }
											        else {
											            cw = pid_cw0[n/2] & 0x0f;
											            tmp = w_den[0][cw];
											            cw = pid_cw1[n/2] & 0x0f;
											            tmp = fast_logmath_add(s->lmath_8b, tmp, w_den[1][cw]);
											            cw = pid_cw2[n/2] & 0x0f;
											            tmp = fast_logmath_add(s->lmath_8b, tmp, w_den[2][cw]);
											            cw = pid_cw3[n/2] & 0x0f;
											            tmp = fast_logmath_add(s->lmath_8b, tmp, w_den[3][cw]);
											        }
											        senone_scores[n] += tmp;
											        l = n;
											    	
											    }
											}
										}
									}
									
									
								}//end ps_mgau_frame_eval
							}
							fsgs->n_sen_eval += acmod->n_senone_active;
							/**
							 * Change the senone score array for a context.
							 **/
							hmm_context_set_senscore(fsgs->hmmctx, senscr); //fsgs->hmmctx->senscore = senscr
							
							/* Mark backpointer table for current frame. */
							//对当前帧 标记后向指针链表的起始索引ID
    						fsgs->bpidx_start = fsg_history_n_entries(fsgs->history);
    						
    						/* Evaluate all active pnodes (HMMs) */
    						//评估所有激活的节点，在函数 fsg_search_start 会对 fsgs->pnode_active fsgs->pnode_active_next 初始化
    						// fsgs->pnode_active  = fsgs->pnode_active_next;
    						//Executed once per frame.
    						fsg_search_hmm_eval(fsgs);
    						{
    							//处理激活的节点
    							fsgs->pnode_active
    							
    							for (n = 0, gn = fsgs->pnode_active; gn; gn = gnode_next(gn), n++) {
    								score = hmm_vit_eval(hmm);
    								
    								if (score BETTER_THAN bestscore)
            							bestscore = score;
    							}
    							
    							
    							fsgs->bestscore = bestscore;
    						}
    						
    						/*
						     * Prune and propagate the HMMs evaluated; create history entries for
						     * word exits.  The words exits are tentative, and may be pruned; make
						     * the survivors permanent via fsg_history_end_frame().
						     */
						     
						     //裁剪 增植隐马模型
						     //创建存在的单词的历史进入点
						     //词的存在是占时的，并且是可裁剪的
						     //通过 fsg_history_end_frame 使幸存下来的词永久保留
						    fsg_search_hmm_prune_prop(fsgs);
						    {
							    thresh = fsgs->bestscore + fsgs->beam;
							    phone_thresh = fsgs->bestscore + fsgs->pbeam;
							    word_thresh = fsgs->bestscore + fsgs->wbeam;
							    
							    //根据以上三个值，判断当前帧的每个节点是否保留
							    for (gn = fsgs->pnode_active; gn; gn = gnode_next(gn)) {
							        pnode = (fsg_pnode_t *) gnode_ptr(gn);
							        hmm = fsg_pnode_hmmptr(pnode);
							        
							        if (hmm_bestscore(hmm) >= thresh) 
							        {
							        	/* Keep this HMM active in the next frame */
							        	//如果帧号没问题，保留当前活动节点到下一帧
							        	if (hmm_frame(hmm) == fsgs->frame) {
							                hmm_frame(hmm) = fsgs->frame + 1;
							                fsgs->pnode_active_next =
							                    glist_add_ptr(fsgs->pnode_active_next,
							                                  (void *) pnode);
							            }
							            else {
							                assert(hmm_frame(hmm) == fsgs->frame + 1);
							            }
							        	
							        	//判断是否是叶子节点
							        	//
							        	if (!fsg_pnode_leaf(pnode)) {
							        		//不是叶子节点，将他的子节点的内容，如果符合条件，也保留激活转态到下一帧
							                if (hmm_out_score(hmm) >= phone_thresh) {
							                    /* Transition out of this phone into its children */
							                    fsg_search_pnode_trans(fsgs, pnode);
							                }
							            }
							            else {
							            	//是叶子节点，转换叶子节点到FSG的状态
							            	//因为单词结束，所以对于这条路径，创建历史表进入点 
							                if (hmm_out_score(hmm) >= word_thresh) {
							                    /* Transition out of leaf node into destination FSG state */
							                    fsg_search_pnode_exit(fsgs, pnode);
							                }
							            }
							        }
							        
							        
							    }
						    }
						    fsg_history_end_frame(fsgs->history);
						    {
						    	//对满足条件的节点添加进历史入口点列表（h->entries）
							    ns = fsg_model_n_state(h->fsg);  //306
							    np = h->n_ciphone;	//70
							
							    for (s = 0; s < ns; s++) {
							        for (lc = 0; lc < np; lc++) {
							            for (gn = h->frame_entries[s][lc]; gn; gn = gnode_next(gn)) {
							                entry = (fsg_hist_entry_t *) gnode_ptr(gn);
							                blkarray_list_append(h->entries, (void *) entry);
							            }
							
							            glist_free(h->frame_entries[s][lc]);
							            h->frame_entries[s][lc] = NULL;
							        }
							    }
						    }
						    
						    
						    /*
						     * Propagate new history entries through any null transitions, creating
						     * new history entries, and then make the survivors permanent.
						     */
						    fsg_search_null_prop(fsgs);
						    {
						    
						    }
						    fsg_history_end_frame(fsgs->history);
						    {
						    }
						    
						    /*
						     * Perform cross-word transitions; propagate each history entry across its
						     * terminating state to the root nodes of the lextree attached to the state.
						     */
						    fsg_search_word_trans(fsgs);
						    
						    
						    /*
						     * We've now come full circle, HMM and FSG states have been updated for
						     * the next frame.
						     * Update the active lists, deactivate any currently active HMMs that
						     * did not survive into the next frame
						     */
						    for (gn = fsgs->pnode_active; gn; gn = gnode_next(gn)) {
						    	
						    	
						    }
						    
						    /* Free the currently active list */
    						glist_free(fsgs->pnode_active);
    						
    						
    						/* Make the next-frame active list the current one */
						    fsgs->pnode_active = fsgs->pnode_active_next;
						    fsgs->pnode_active_next = NULL;
						
						    /* End of this frame; ready for the next */
						    ++fsgs->frame;
						}
					}
					
					//更新各种计数值
					acmod_advance(acmod);
			        ++ps->n_frame;
			        ++nfr;
				}
				
				return nfr;
			}
		}
	}
	
	
/*
	typedef struct fsg_link_s {
	    int32 from_state;
	    int32 to_state;
	    int32 logs2prob;	/**< log(transition probability)*lw */
	    int32 wid;		/**< Word-ID; <0 if epsilon or null transition */
	} fsg_link_t;
*/

	ps_get_hyp
	{
		//计算时间
		ptmr_start
		
		ps_search_hyp--》  fsg_search_hyp
		{
			///* Get last backpointer table index. */
			//从所有可能的路径中找出路径评分最优的路径索引值
			bp = fsg_search_find_exit
			{
				/* Scan backwards to find a word exit in frame_idx. */
				// 反向扫描，找到单词结束的帧号，即最后一个词的结束的帧号
				fsg_history_n_entries
				
				/* Now find best word exit in this frame. */
				// 从所有可能的路径中找出路径评分最优的路径索引值
				
				/* This here's the one we want. */
				// 更新评分  或者 更新输出结果的属性--语法是否已经结束（即是否完整）
			    if (out_score)
			        *out_score = bestscore;
			        
			    if (out_is_final) {}
			}
			
			//通过 bp(bestPath) 最优路径找到 该路径的单词 ，并作词过滤
			//第一遍计算 该路径的文本长度
			while (bp > 0) {
				fsg_hist_entry_t *hist_entry = fsg_history_entry_get(fsgs->history, bp);
        		fsg_link_t *fl = fsg_hist_entry_fsglink(hist_entry);
        		
        		bp = fsg_hist_entry_pred(hist_entry);
			}
			
			//第二遍复制单词，获得结果
			while (bp > 0) {
			}
			
			
		}
		
		ptmr_stop
	}
	
2015-3-11	
	阅读代码总结：
		1.注意向引擎传递数据的量，最大帧数 7，1 *200 + (7 - 1) * 80 = 680	----有时间必须验证一下？？？？？
		  尽可能避免每次传递数据都向缓冲区写，最好按帧数取数据，保证不会有溢出，也就不用处理溢出的情况
		  
		2.//考虑将每次写入数据都进行评分搜索改成，最后进行一次 ---- 有时间必须验证一下？？？？？
			ps_search_forward
			
		3.尝试改变 "-topn" 的默认值（4），
			改成3，理论上计算量高斯评分计算量并未减少，但是后续对后续搜索路径减少很多的可能性，
			如果识别率没有太大变化，可以用来尝试提升速度，
			反之，如果希望提高识别率，理论上可以将4改成5，或者更大的值
			
			
		acmod_write_scores 记录当前的音素以及音素的评分
			
			
2015嵌入式识别引擎的工作计划 2015-3-27
	按优先级排列：
	1.测试各种参数对引擎性能的影响
	2.测试引擎在各种性能移动端的性能表现
	3.解决测试过程中发现的影响引擎稳定问题
	4.iOS版本嵌入式引擎的SDK,五月份有项目需求
	5.引擎词典的优化，根据语法文件自动分词之后，生成新的词典，目前加载大辞典对引擎初始化的速度影响太大
	6.模型的浮点转定点，识别引擎浮点转定点	
	7.引擎速度的持续优化，争取500条数据的实时率在一以内，1000条数据的实时率在2以内
	8.在嵌入式引擎中使用神经网络